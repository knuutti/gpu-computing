{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq9Q99nQP7y-"
      },
      "source": [
        "# GPU Computing - Exercise 2\n",
        "## Eetu Knutars\n",
        "## 21.1.2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOcncoiDQG2s"
      },
      "source": [
        "### Task 1 + 2\n",
        "Implement vector differentiation similar to NumPy diff function. Extend the kernel to use shared memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfOjwe06q-V5"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RQTvJfvsNIV8"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6QXne92q-V6"
      },
      "source": [
        "Defining the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zO0lAeSMyQuM"
      },
      "outputs": [],
      "source": [
        "# Define the CUDA kernel for differentiation using shared memory\n",
        "diff_kernel = cp.RawKernel(r'''extern \"C\"\n",
        "__global__ void diff_shared(const double* input, double* output, int array_size) {\n",
        "    extern __shared__ double shared_input[];\n",
        "\n",
        "    int thread_index = threadIdx.x;\n",
        "    int global_index = blockIdx.x * blockDim.x + thread_index;\n",
        "\n",
        "    // Load elements into shared memory\n",
        "    if (global_index < array_size) {\n",
        "        shared_input[thread_index] = input[global_index];\n",
        "    }\n",
        "\n",
        "    // Load the next element for the last thread in the block (if within bounds)\n",
        "    if (thread_index == blockDim.x - 1 && global_index < array_size - 1) {\n",
        "        shared_input[thread_index + 1] = input[global_index + 1];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Compute differences using shared memory\n",
        "    if (global_index < array_size - 1) {\n",
        "        output[global_index] = shared_input[thread_index + 1] - shared_input[thread_index];\n",
        "    }\n",
        "}''', 'diff_shared')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjz5nCylq-V6"
      },
      "source": [
        "Generate the vectors and call the kernel to calculate difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzuzn6TkyhhZ",
        "outputId": "5026501e-f6c2-496d-dd80-2b6b0eee24d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They are the same!\n"
          ]
        }
      ],
      "source": [
        "def diff_cupy_shared(input_array):\n",
        "    \"\"\"\n",
        "    Perform differentiation similar to NumPy's diff function using CuPy and shared memory.\n",
        "\n",
        "    Parameters:\n",
        "        input_array (cp.ndarray): Input 1D array.\n",
        "\n",
        "    Returns:\n",
        "        cp.ndarray: Output array of differences.\n",
        "    \"\"\"\n",
        "    n = input_array.size\n",
        "    if n < 2:\n",
        "        raise ValueError(\"Input array must have at least two elements.\")\n",
        "\n",
        "    # Allocate memory for the output array\n",
        "    output_array = cp.zeros(n - 1, dtype=cp.float64)\n",
        "\n",
        "    # Define block and grid sizes\n",
        "    threads_per_block = 256\n",
        "    blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n",
        "\n",
        "    # Shared memory size (threads_per_block * sizeof(double))\n",
        "    shared_mem_size = threads_per_block * cp.float64().itemsize\n",
        "\n",
        "    # Launch the kernel\n",
        "    diff_kernel((blocks_per_grid,), (threads_per_block,),\n",
        "                (input_array, output_array, n),\n",
        "                shared_mem=shared_mem_size)\n",
        "\n",
        "    return output_array\n",
        "\n",
        "input = cp.array([1., 2., 0., 4., 5.])\n",
        "if (np.allclose(diff_cupy_shared(input), np.diff(input), 0.001, 0.001)):\n",
        "  print(\"They are the same!\")\n",
        "else:\n",
        "  print(\"Something must have gone wrong.:(\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXjZ_mTWQbKX"
      },
      "source": [
        "# Task 3\n",
        "Extend the matrix-matrix multiplication kernel from lectures to use shared memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAKS2f3Zq-V7"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Kliwj5TQ1yg"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKVX3f8lq-V7"
      },
      "source": [
        "Defining the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FB7BDXpIQ2uw"
      },
      "outputs": [],
      "source": [
        "our_kernel = cp.RawKernel(r''' extern \"C\"\n",
        "#define TILE_DIM 32\n",
        "\n",
        "__global__ void matrix_multiplication_shared_memory(const double* A, const double* B, double* C, int N, int M, int K)\n",
        "{\n",
        "\n",
        "\t  // Allocate the sub-matrices to the shared memory. Note two-dim indexing.\n",
        "    __shared__ double sub_A[TILE_DIM][TILE_DIM];\n",
        "    __shared__ double sub_B[TILE_DIM][TILE_DIM];\n",
        "\n",
        "    const int tx = threadIdx.x;\n",
        "    const int ty = threadIdx.y;\n",
        "\n",
        "    const int bx = blockIdx.x;\n",
        "    const int by = blockIdx.y;\n",
        "\n",
        "    // Each block gets it own TILE_DIM sized slot in x and y directions.\n",
        "    const int row = by * TILE_DIM + ty;\n",
        "    const int col = bx * TILE_DIM + tx;\n",
        "\n",
        "    double result = 0.0;\n",
        "\n",
        "    for(int i = 0; i <  M  / TILE_DIM; i++)\n",
        "        {\n",
        "\n",
        "        // Iterate over the tile dimension to copy the data.\n",
        "        sub_A[ty][tx] = A[(i * TILE_DIM + tx) + M * row];\n",
        "        sub_B[ty][tx] = B[(i * TILE_DIM + ty) * K + col];\n",
        "\n",
        "\t\t    // Make sure that all threads have completed the memory transaction.\n",
        "        __syncthreads();\n",
        "\n",
        "        // Multiply the matrix elements inside the tile and add them to the result.\n",
        "        for (int j = 0; j < TILE_DIM; j++)\n",
        "            {\n",
        "            result += sub_A[ty][j] * sub_B[j][tx];\n",
        "            }\n",
        "\n",
        "\t\t    // Make sure that all of the threads have finished the calculation\n",
        "        __syncthreads();\n",
        "        }\n",
        "\n",
        "    // Write back to the global memory. Using the global index.\n",
        "    int C_index = K * (by * blockDim.y + ty) + (bx * blockDim.x + tx);\n",
        "    C[C_index] = result;\n",
        "}\n",
        "''', 'matrix_multiplication_shared_memory')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zqY5pysq-V8"
      },
      "source": [
        "Generating the input matrices and calling the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fqG8DslQQ-x_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9052f29b-13dc-4e7f-fb46-1f9ea00ff489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They are the same!\n"
          ]
        }
      ],
      "source": [
        "# Define matrix dimensions\n",
        "A_n_rows = 1024\n",
        "A_n_cols = 512\n",
        "B_n_rows = 512\n",
        "B_n_cols = 256\n",
        "C_n_rows = A_n_rows\n",
        "C_n_cols = B_n_cols\n",
        "\n",
        "value_type = cp.float64\n",
        "\n",
        "# Define the block and grid sizes\n",
        "numThreadsPerBlock = 32\n",
        "numBlocksx = math.ceil(C_n_cols/numThreadsPerBlock)\n",
        "numBlocksy = math.ceil(C_n_rows/numThreadsPerBlock)\n",
        "\n",
        "# Generate the matrices\n",
        "A = cp.random.randn(A_n_rows, A_n_cols)\n",
        "A = A.astype(value_type)\n",
        "\n",
        "B = cp.random.randn(B_n_rows, B_n_cols)\n",
        "B = B.astype(value_type)\n",
        "\n",
        "C = cp.zeros([C_n_rows,C_n_cols])\n",
        "C = C.astype(value_type)\n",
        "\n",
        "# Call the CUDA kernel for matrix multiplication\n",
        "our_kernel((numBlocksx, numBlocksy,1),(numThreadsPerBlock,numThreadsPerBlock, 1),(A, B, C, np.int32(A_n_rows), np.int32(A_n_cols), np.int32(B_n_cols)))\n",
        "\n",
        "# Check the result by comparing it to the NumPy result\n",
        "C_cupy = cp.dot(A,B)\n",
        "if (np.allclose(C_cupy, C, 0.001, 0.001)):\n",
        "  print(\"They are the same!\")\n",
        "else:\n",
        "  print(\"Something must have gone wrong.:(\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}